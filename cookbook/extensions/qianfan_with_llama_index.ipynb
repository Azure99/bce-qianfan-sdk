{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在 Llama Index 中使用千帆能力\n",
    "\n",
    "LlamaIndex是一个用于连接大语言模型（LLMs）和外部数据源的数据框架。它能够让LLMs访问和利用私有或领域特定的数据，以优化模型性能并使其更加易用和流畅。\n",
    "\n",
    "本文准备了一份在 Llama Index 中使用千帆的能力进行 RAG 的示例，供用户参考。\n",
    "\n",
    "# 前置准备\n",
    "\n",
    "用户首先需要自行安装集成了 OpenAI 组件的 Llama Index，以及安装最新版本的千帆 Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U llama-index\n",
    "!pip install -U \"qianfan[openai]\"\n",
    "!pip install -U nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在安装完成之后，我们还需要在 Shell 中设置相关环境变量并启动千帆的 OpenAI 转接服务。这个服务负责监听本地发出的、指向 OpenAI 的请求，并将请求映射并转发到千帆，最后从千帆拿回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI wrapper server is running at                                             \n",
      "\n",
      "\u001b[1;33m • \u001b[0mhttp://127.0.0.1:8001                                                        \n",
      "\u001b[1;33m • \u001b[0mhttp://172.18.183.188:8001                                                   \n",
      "\n",
      "Remember to set the environment variables:                                      \n",
      "\n",
      "\u001b[40m                                                                                \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[97;40m    \u001b[0m\u001b[97;40mexport\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mOPENAI_API_KEY\u001b[0m\u001b[91;40m=\u001b[0m\u001b[93;40m'any-content-you-want'\u001b[0m\u001b[40m                              \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[97;40m    \u001b[0m\u001b[97;40mexport\u001b[0m\u001b[97;40m \u001b[0m\u001b[97;40mOPENAI_BASE_URL\u001b[0m\u001b[91;40m=\u001b[0m\u001b[93;40m'http://172.18.183.188:8001/v1'\u001b[0m\u001b[40m                    \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m                                                                                \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [44989]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "import subprocess as sp\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"FAKE_KEY\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8001/v1\"\n",
    "os.environ[\"QIANFAN_ACCESS_KEY\"] = \"your_qianfan_console_access_key\"\n",
    "os.environ[\"QIANFAN_SECRET_KEY\"] = \"your_qianfan_console_secret_key\"\n",
    "os.environ[\"QIANFAN_QPS_LIMIT\"] = \"5\"\n",
    "\n",
    "server = sp.Popen(\"qianfan openai -p 8001\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据与构建向量库\n",
    "\n",
    "Llama Index 提供的数据加载功能允许我们直接从文件夹方便的构建向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengyiyang/miniconda3/envs/bce-qianfan-sdk-new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (31) is close to chunk size (50). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "Generating embeddings:   0%|          | 0/1151 [00:00<?, ?it/s][INFO] [03-28 18:48:12] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   1%|          | 10/1151 [00:01<03:03,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:13] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   2%|▏         | 20/1151 [00:02<01:55,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:14] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   3%|▎         | 30/1151 [00:02<01:38, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:14] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   3%|▎         | 40/1151 [00:03<01:22, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:15] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   4%|▍         | 50/1151 [00:04<01:15, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:15] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   5%|▌         | 60/1151 [00:04<01:11, 15.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:16] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   6%|▌         | 70/1151 [00:05<01:13, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:17] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   7%|▋         | 80/1151 [00:05<01:10, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:17] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   8%|▊         | 90/1151 [00:06<01:12, 14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:18] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:   9%|▊         | 100/1151 [00:07<01:12, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:19] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  10%|▉         | 110/1151 [00:08<01:12, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:20] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  10%|█         | 120/1151 [00:08<01:09, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:20] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  11%|█▏        | 130/1151 [00:09<01:10, 14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:21] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  12%|█▏        | 140/1151 [00:10<01:09, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:22] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  13%|█▎        | 150/1151 [00:10<01:04, 15.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:22] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  14%|█▍        | 160/1151 [00:11<01:05, 15.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:23] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  15%|█▍        | 170/1151 [00:12<01:03, 15.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:23] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  16%|█▌        | 180/1151 [00:12<01:02, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:24] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  17%|█▋        | 190/1151 [00:13<01:00, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:25] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  17%|█▋        | 200/1151 [00:13<00:59, 15.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:25] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  18%|█▊        | 210/1151 [00:14<00:58, 16.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:26] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  19%|█▉        | 220/1151 [00:15<01:00, 15.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:27] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  20%|█▉        | 230/1151 [00:15<00:59, 15.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:27] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  21%|██        | 240/1151 [00:16<00:57, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:28] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  22%|██▏       | 250/1151 [00:16<00:55, 16.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:28] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  23%|██▎       | 260/1151 [00:17<00:53, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:29] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  23%|██▎       | 270/1151 [00:18<00:52, 16.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:30] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  24%|██▍       | 280/1151 [00:18<00:52, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:30] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  25%|██▌       | 290/1151 [00:19<00:52, 16.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:31] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  26%|██▌       | 300/1151 [00:20<00:54, 15.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:32] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  27%|██▋       | 310/1151 [00:20<00:49, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:32] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  28%|██▊       | 320/1151 [00:21<00:50, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:33] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  29%|██▊       | 330/1151 [00:21<00:49, 16.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:33] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  30%|██▉       | 340/1151 [00:22<00:50, 16.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:34] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  30%|███       | 350/1151 [00:23<00:51, 15.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:35] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  31%|███▏      | 360/1151 [00:23<00:50, 15.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:35] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  32%|███▏      | 370/1151 [00:24<00:49, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:36] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  33%|███▎      | 380/1151 [00:25<00:47, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:36] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  34%|███▍      | 390/1151 [00:25<00:47, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:37] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  35%|███▍      | 400/1151 [00:26<00:44, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:38] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  36%|███▌      | 410/1151 [00:26<00:46, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:38] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  36%|███▋      | 420/1151 [00:27<00:45, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:39] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  37%|███▋      | 430/1151 [00:28<00:46, 15.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:40] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  38%|███▊      | 440/1151 [00:28<00:47, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:40] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  39%|███▉      | 450/1151 [00:29<00:47, 14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:41] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  40%|███▉      | 460/1151 [00:30<00:45, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:42] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  41%|████      | 470/1151 [00:30<00:42, 15.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:42] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  42%|████▏     | 480/1151 [00:31<00:41, 16.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:43] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  43%|████▎     | 490/1151 [00:31<00:39, 16.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:43] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  43%|████▎     | 500/1151 [00:32<00:38, 17.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:44] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  44%|████▍     | 510/1151 [00:33<00:42, 15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:45] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  45%|████▌     | 520/1151 [00:33<00:40, 15.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:45] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  46%|████▌     | 530/1151 [00:34<00:41, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:46] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  47%|████▋     | 540/1151 [00:35<00:41, 14.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:47] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  48%|████▊     | 550/1151 [00:35<00:39, 15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:47] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  49%|████▊     | 560/1151 [00:36<00:43, 13.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:48] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  50%|████▉     | 570/1151 [00:37<00:38, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:49] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  50%|█████     | 580/1151 [00:38<00:37, 15.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:49] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  51%|█████▏    | 590/1151 [00:38<00:36, 15.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:50] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  52%|█████▏    | 600/1151 [00:39<00:35, 15.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:51] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  53%|█████▎    | 610/1151 [00:39<00:35, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:51] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  54%|█████▍    | 620/1151 [00:40<00:36, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:52] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  55%|█████▍    | 630/1151 [00:41<00:34, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:53] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  56%|█████▌    | 640/1151 [00:42<00:34, 14.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:53] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  56%|█████▋    | 650/1151 [00:42<00:32, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:54] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  57%|█████▋    | 660/1151 [00:43<00:31, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:55] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  58%|█████▊    | 670/1151 [00:43<00:32, 14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:55] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  59%|█████▉    | 680/1151 [00:44<00:32, 14.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:56] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  60%|█████▉    | 690/1151 [00:45<00:31, 14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:57] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  61%|██████    | 700/1151 [00:46<00:30, 14.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:57] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  62%|██████▏   | 710/1151 [00:46<00:29, 14.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:58] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  63%|██████▎   | 720/1151 [00:47<00:29, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:48:59] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  63%|██████▎   | 730/1151 [00:48<00:28, 14.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:00] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  64%|██████▍   | 740/1151 [00:48<00:26, 15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:00] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  65%|██████▌   | 750/1151 [00:49<00:26, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:01] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  66%|██████▌   | 760/1151 [00:50<00:25, 15.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:01] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  67%|██████▋   | 770/1151 [00:50<00:24, 15.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:02] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  68%|██████▊   | 780/1151 [00:51<00:23, 15.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:03] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  69%|██████▊   | 790/1151 [00:51<00:22, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:03] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  70%|██████▉   | 800/1151 [00:52<00:21, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:04] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  70%|███████   | 810/1151 [00:53<00:21, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:04] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  71%|███████   | 820/1151 [00:53<00:20, 16.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:05] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  72%|███████▏  | 830/1151 [00:54<00:20, 15.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:06] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  73%|███████▎  | 840/1151 [00:55<00:20, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:07] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  74%|███████▍  | 850/1151 [00:55<00:19, 15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:07] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  75%|███████▍  | 860/1151 [00:56<00:17, 16.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:08] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  76%|███████▌  | 870/1151 [00:56<00:17, 15.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:08] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  76%|███████▋  | 880/1151 [00:57<00:16, 16.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:09] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  77%|███████▋  | 890/1151 [00:58<00:16, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:10] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  78%|███████▊  | 900/1151 [00:58<00:15, 16.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:10] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  79%|███████▉  | 910/1151 [00:59<00:14, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:11] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  80%|███████▉  | 920/1151 [00:59<00:14, 16.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:11] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  81%|████████  | 930/1151 [01:00<00:13, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:12] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  82%|████████▏ | 940/1151 [01:01<00:14, 14.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:13] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  83%|████████▎ | 950/1151 [01:02<00:13, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:13] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  83%|████████▎ | 960/1151 [01:02<00:12, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:14] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  84%|████████▍ | 970/1151 [01:03<00:12, 14.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:15] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  85%|████████▌ | 980/1151 [01:03<00:10, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:15] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  86%|████████▌ | 990/1151 [01:04<00:10, 15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:16] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  87%|████████▋ | 1000/1151 [01:05<00:09, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:17] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  88%|████████▊ | 1010/1151 [01:05<00:08, 15.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:17] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  89%|████████▊ | 1020/1151 [01:06<00:08, 15.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:18] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  89%|████████▉ | 1030/1151 [01:07<00:08, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:19] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  90%|█████████ | 1040/1151 [01:07<00:06, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:19] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  91%|█████████ | 1050/1151 [01:08<00:05, 16.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:20] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  92%|█████████▏| 1060/1151 [01:09<00:05, 15.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:20] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  93%|█████████▎| 1070/1151 [01:09<00:04, 16.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:21] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  94%|█████████▍| 1080/1151 [01:10<00:04, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:22] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  95%|█████████▍| 1090/1151 [01:10<00:03, 15.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:22] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  96%|█████████▌| 1100/1151 [01:11<00:03, 16.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:23] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  96%|█████████▋| 1110/1151 [01:12<00:02, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:24] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  97%|█████████▋| 1120/1151 [01:12<00:01, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:24] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  98%|█████████▊| 1130/1151 [01:13<00:01, 16.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:25] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings:  99%|█████████▉| 1140/1151 [01:13<00:00, 16.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:25] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings: 100%|█████████▉| 1150/1151 [01:14<00:00, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:49:26] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n",
      "Generating embeddings: 100%|██████████| 1151/1151 [01:15<00:00, 15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58923 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "transformations=[\n",
    "    SentenceSplitter(chunk_size=50, chunk_overlap=0),\n",
    "    OpenAIEmbedding(embed_batch_size=10),\n",
    "]\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=transformations, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查询文本\n",
    "\n",
    "在构建完成后，`index` 对象就是一个包含了上面文本的向量化表示的向量数据库。我们可以直接使用该向量数据库所提供的大模型查询方法 `query` ，用自然语言来查询向量数据库中的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:50:29] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /embeddings/embedding-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59061 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 18:50:30] openapi_requestor.py:377 [t:8094817088]: async requesting llm api endpoint: /chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59063 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\n",
      "大模型回答: Based on the provided context information, there is no mention of what the author did growing up. The context only provides information about a file path and excerpts from a text file named \"paul_graham_essay.txt\". Therefore, it is not possible to answer the query based on the given context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(f\"\\n大模型回答: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后关闭我们启动的服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [44989]\n"
     ]
    }
   ],
   "source": [
    "server.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bce-qianfan-sdk-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
