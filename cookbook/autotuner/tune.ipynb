{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型推理配置自动推荐\n",
    "\n",
    "千帆平台上提供了大量模型，同时每个模型都提供了丰富的参数可供用户调整以适应不同的应用场景。然而，挑选合适的模型以及为模型挑选合适的参数是一件十分繁琐的事，需要不断进行尝试，并且在不同场景下，最优的配置也不尽相同，需要反复。\n",
    "\n",
    "针对这个问题，SDK 提供了推理配置自动推荐的功能，只需提供您的目标场景数据集和评估标准，设定您的搜索空间，我们的SDK便能智能推荐最优的模型及其参数配置。这一全新功能确保了在不同场景下，您都能轻松找到或优化至理想的模型配置，释放模型潜能，提升效能至极致。\n",
    "\n",
    "接下来，在本 cookbook 中将展示如何使用 SDK 中的推理配置推荐功能，并展示该功能所带来的性能提升。\n",
    "\n",
    "**注意**：这个功能需要千帆 SDK 版本 >= 0.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qianfan\n",
    "\n",
    "os.environ[\"QIANFAN_ACCESS_KEY\"] = \"your access key\"\n",
    "os.environ[\"QIANFAN_SECRET_KEY\"] = \"your secret key\"\n",
    "# 由于后续在调优配置的过程中会并发请求模型，建议限制 QPS 和重试次数，避免调用失败\n",
    "os.environ[\"QIANFAN_QPS_LIMIT\"] = \"3\"\n",
    "os.environ[\"QIANFAN_LLM_API_RETRY_COUNT\"] = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "\n",
    "具体而言，在获取推荐配置前，需要先准备：\n",
    "\n",
    "- 数据集：根据目标场景准备一定量的数据\n",
    "- 评估方式：根据目标场景，选择待优化的指标，并提供评估函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集使用的是千帆 SDK 中提供的 `Dataset` 模块，可以直接加载本地的数据集文件，也可以使用平台上预置的或者自行上传的数据集，具体加载方式参考 [文档](https://github.com/baidubce/bce-qianfan-sdk/blob/main/docs/dataset.md)。\n",
    "\n",
    "这里我们以一个 **角色扮演** 数据集为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qianfan.dataset import Dataset\n",
    "\n",
    "dataset = Dataset.load(\n",
    "    data_file=\"./example.jsonl\",\n",
    "    organize_data_as_group=False,\n",
    "    input_columns=[\"prompt\"],\n",
    "    reference_column=\"response\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [04-01 11:29:12] dataset.py:883 [t:140289325633792]: list local dataset data by 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'姓名': '白展堂', '性别': '男', '物种': '人类', '年龄': '25岁', '工作': '同福客栈跑堂、七侠镇捕快兼口头行动负责人（临时）、同福武馆馆长（已解散）', '昵称': '白玉汤、盗圣、展堂、老白、白玉展堂、堂堂（小名）、白大哥、白二哥（姬无病称呼）、黄豆豆、草头王、王豆豆、草头黄先生、干大爷、大舅哥、能喝八两绝不喝半斤、杀千刀的老贼、贼头、红鼻子盗圣、小白、展大哥、老白同志、白少侠、盗圣哥哥、我滴儿、白捕头、姐夫、大佬、白展胖、白先生、好女婿、乖女婿', '身高': '1.83 m', '体重': '140 斤', '居住地': '东北', '恋爱状态': '已婚，妻子是佟湘玉', '爱好': '钱（或者珠宝）、说书或听书、赌博（曾经）、唱歌、喝酒、画画、医术', '学历': '上等文化', '其他': '喜欢财宝，有贼性，但愿意为朋友两肋插刀，有一定的医术知识', '经典台词': ['天下第二，第一是楚留香，比赛那天我光着脚，而且是顶风。', '你说的这是我那没脸没皮好吃懒做的白大哥吗？', '紧张的时候咬手指，一个不够得啃俩。', '练武是用来比的吗？是在真正需要它的时候派上用场。强盗来了你可以迎头痛击，路见不平你可以拔刀相助。只有在这些时候，武功才能发挥出它真正的威力。而这威力的源泉，都是来源于你心中的一股正气。一个人，只要他有了这股正气，即使他没有武功，同样可以天下无敌！', '此恨绵绵无期绝~', '天若有情天亦老~', '我真是少爷的身子跑堂的命。', '站在天堂看地狱，人生就像情景剧，站在地狱看天堂，为谁辛苦为谁忙。', '分手总是在雨天!', '客官，首先请接受我诚挚的祝福', '只要给够加班费，当牛做马无所谓!', '找点哪!', '葵花点穴手!', '再来劲小心我点你'], '口头禅': ['客官您里边请!', '此恨绵绵无期绝~', '天若有情天亦老~', '找点哪!', '葵花点穴手!'], '人物经历': '白展堂，原名白玉汤，葵花派弟子，江湖上的盗圣。后厌倦江湖生活，落脚同福客栈，改名白展堂。曾有一段初恋情感经历，最终与佟湘玉结为夫妻。在同福客栈中，他以其独特的个性和机智幽默成为了不可或缺的一员。', '人物关系': '父亲周氏，母亲白三娘（白翠萍），妻子佟湘玉，师父葵花派东西南北四大长老，师妹祝无双，兄弟般的同事李大嘴，发小盗神姬无命。', '喜欢的事情/东西': '喜欢钱财、珠宝、说书听书、唱歌、喝酒、画画、医术。', '不喜欢的事情/东西': '不喜欢捕头捕快，对捕快有抵触情绪。', '人物性格': '性子爽朗，明白事理，有责任心，胆小，贪图享受，善于撒谎，有时候喜欢不分场合乱说话，跟谁都假熟，没脸没皮好吃懒做。', '武功': '葵花点穴手、葵花解穴手、轻功（白三娘传授，天下第二）、隔空打穴、九九还阳掌（对佟湘玉使过）、分筋错骨手（对李大嘴使过，但未说明）', '必杀技': '以眼电人（仅限佟湘玉）、爱情三十六计（跟佟湘玉斗气时向吕轻侯口述过，其实都是无心的）', '克星': '佟湘玉、白三娘、六扇门（获得免罪金牌前）、上官云顿、展红绫（曾经，确切的说是害怕她穿的捕快的衣服）、四大神捕、带笑脸的葵花盘（曾经）、葵花派（曾经）、葵花派四大长老（曾经）郭巨侠（可没收免罪金牌）公孙乌龙、杜子俊的娘', '结局': '与佟掌柜成亲。'}\n",
      "现在请你扮演一个角色扮演专家。请你根据上述信息扮演白展堂进行对话。\n",
      "\n",
      "好的！现在我来扮演白展堂。我首先发话：来，汤圆好了，吃汤圆。\n",
      "展红绫：有了，你骑过马没有。那种野马，骑上去以后，一面担心摔下来，一面又不想下来，就想那么一直骑下去，心里一直跳呀跳呀。\n",
      "白展堂：你忽略了一点啊，再野的马也有被驯服的那一天。\n",
      "展红绫：可是...\n",
      "白展堂：现在我们进行第二个问题，你觉得追风跟我比他差在哪儿。\n",
      "展红绫：他比你强得多，各方面不是一星半点儿。\n",
      "白展堂：可是他没有我那么刺激。\n",
      "展红绫：刺激？也许是吧。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in dataset[0]:\n",
    "    print(item['prompt'])\n",
    "    print(item['response'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估采用的 SDK 提供的 Evaluator 模块，基于 Evaluator 实现 evaluate 方法即可。如下实现了一个利用大模型评分实现评估的 Evaluator，关于如何实现 Evaluator 可以参考 [该cookbook](https://github.com/baidubce/bce-qianfan-sdk/blob/main/cookbook/evaluation/local_eval_with_qianfan.ipynb)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qianfan.evaluation.evaluator import LocalEvaluator\n",
    "from qianfan import ChatCompletion\n",
    "from qianfan.common.prompt.prompt import Prompt\n",
    "from qianfan.utils.pydantic import Field\n",
    "\n",
    "from typing import Optional, Union, Any, Dict, List\n",
    "import re\n",
    "import json\n",
    "\n",
    "class LocalJudgeEvaluator(LocalEvaluator):\n",
    "    model: Optional[ChatCompletion] = Field(default=None, description=\"model object\")\n",
    "    cache: Dict[str, Any] = Field(default={}, description=\"cache for evaluation\")\n",
    "    eval_prompt: Prompt = Field(\n",
    "        default=Prompt(\n",
    "            template=\"\"\"你需要扮演一个裁判的角色，对一段角色扮演的对话内容进行打分，你需要考虑这段文本中的角色沉浸度和对话文本的通畅程度。你可以根据以下规则来进行打分，你可以阐述你对打分标准的理解后再给出分数：\n",
    "                \"4\":完全可以扮演提问中的角色进行对话，回答完全符合角色口吻和身份，文本流畅语句通顺\n",
    "                \"3\":扮演了提问中正确的角色，回答完全符合角色口吻和身份，但文本不流畅或字数不满足要求\n",
    "                \"2\":扮演了提问中正确的角色，但是部分语句不符合角色口吻和身份，文本流畅语句通顺\n",
    "                \"1\":能够以角色的口吻和身份进行一部分对话，和角色设定有一定偏差，回答内容不流畅，或不满足文本字数要求\n",
    "                \"0\":扮演了错误的角色，没有扮演正确的角色，角色设定和提问设定差异极大，完全不满意\n",
    "                你的回答需要以json代码格式输出：\n",
    "                ```json\n",
    "                {\"modelA\": {\"justification\": \"此处阐述对打分标准的理解\", \"score\": \"此处填写打分结果\"}}\n",
    "                ```\n",
    "\n",
    "                现在你可以开始回答了：\n",
    "                问题：{{input}}\n",
    "                ---\n",
    "                modelA回答：{{output}}\n",
    "                ---\"\"\",\n",
    "            identifier=\"{{}}\",\n",
    "        ),\n",
    "        description=\"evaluation prompt\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def evaluate(\n",
    "        self, input: Union[str, List[Dict[str, Any]]], reference: str, output: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        score = 0\n",
    "        try:\n",
    "            # 渲染评估用的 prompt，传入输入、模型输出和参考答案\n",
    "            p, _ = self.eval_prompt.render(\n",
    "                **{\n",
    "                    \"input\": \"\\n\".join([i[\"content\"] for i in input[1:]]),\n",
    "                    \"output\": output,\n",
    "                    \"expect\": reference,\n",
    "                }\n",
    "            )\n",
    "            # 利用 cache 避免对同一结果反复进行评估，提升效率\n",
    "            if p in self.cache:\n",
    "                model_output = self.cache[p]\n",
    "                score = float(model_output[\"modelA\"][\"score\"])\n",
    "            else:\n",
    "                # 请求模型进行评估\n",
    "                r = self.model.do(messages=[{\"role\": \"user\", \"content\": p}], temperature=0.01)\n",
    "                content = r[\"result\"]\n",
    "                model_output = content\n",
    "                # 提取出 json 格式的评估结果\n",
    "                regex = re.compile(\"\\`\\`\\`json(.*)\\`\\`\\`\", re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "                u = regex.findall(content)\n",
    "    \n",
    "                if len(u) == 0:\n",
    "                    score = 0\n",
    "                else:\n",
    "                    model_output = json.loads(u[0])\n",
    "                    score = float(model_output[\"modelA\"][\"score\"])\n",
    "                    self.cache[p] = model_output\n",
    "        except Exception as e:\n",
    "            score = 0\n",
    "        # 返回评估结果，这里字段需与后续推荐配置时设定的评估字段一致\n",
    "        return {\"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取推荐配置\n",
    "\n",
    "为了获取推荐配置，还需要设置一个超参搜索空间，千帆 SDK 提供了如下表示搜索空间的类：\n",
    "\n",
    "- `Uniform`：表示一个均匀分布的搜索空间，包含两个参数 `low` 和 `high`，分别表示下界和上界。\n",
    "- `Categorical`：表示一个离散的搜索空间，包含一个参数 `choices`，表示一组候选值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qianfan.autotuner.space import Uniform, Categorical\n",
    "\n",
    "search_space = {\n",
    "    \"temperature\": Uniform(0.01, 0.99),  # 设定temperature的范围\n",
    "    \"model\": Categorical([\"ERNIE-Speed\", \"ERNIE-Bot-turbo\"]),  # 设定model的取值范围\n",
    "    # 更多其他参数也可以按同样方式设定\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后就可以执行推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qianfan.autotuner\n",
    "\n",
    "context = await qianfan.autotuner.run(\n",
    "    search_space=search_space,\n",
    "    dataset=dataset,\n",
    "    evaluator=LocalJudgeEvaluator(\n",
    "        model=ChatCompletion(model=\"ERNIE-Bot-4\")\n",
    "    ),\n",
    "    # 以下均为可选参数\n",
    "    suggestor=\"random\",  # 搜索算法，目前仅支持 \"random\"，更多算法敬请期待\n",
    "    cost_budget=20,      # 设定整个流程的预算，达到预算则终止流程，单位为 “元”\n",
    "    metrics=\"score\",     # 设定评估指标字段，与 Evaluator 输出对应\n",
    "    mode=\"max\",          # 设定评估指标最大化还是最小化\n",
    "    repeat=5,            # 重复推理次数，用于减少大模型输出随机性对结果准确性的干扰\n",
    "    # max_turn=10,         # 设定最大尝试次数\n",
    "    # max_time=3600,       # 设定最大尝试时间，单位为秒\n",
    "    log_dir= \"./log\",    # 日志目录\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回的结果是一个 `Context` 对象，其中包含了整个搜索过程的所有上下文信息，例如可以通过如下方式获得搜索的最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 0.23879676911568554, 'model': 'ERNIE-Speed'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个参数可以直接用于推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = qianfan.ChatCompletion().do(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"messages\": \"请扮演一个角色，然后说一句话\"\n",
    "}], **context.best)\n",
    "\n",
    "print(chat['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context 中也包含了整个过程中尝试的记录，可以获取某一轮某一组配置的评估结果等信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE-Bot-turbo\t0.9350118283831285\t2.1816326530612247\n",
      "ERNIE-Bot-turbo\t0.37900692272161335\t2.6857142857142855\n",
      "ERNIE-Speed\t0.5047393945107158\t2.8081632653061224\n",
      "ERNIE-Speed\t0.23879676911568554\t3.020408163265306\n",
      "ERNIE-Speed\t0.8166117015206144\t2.5408163265306123\n",
      "ERNIE-Bot-turbo\t0.9271271943982626\t2.1918367346938776\n",
      "ERNIE-Bot-turbo\t0.8600257278657117\t2.23265306122449\n",
      "ERNIE-Bot-turbo\t0.3153142670496584\t2.563265306122449\n",
      "ERNIE-Speed\t0.47733409897219253\t2.8183673469387753\n",
      "ERNIE-Bot-turbo\t0.9237762484049076\t2.263265306122449\n",
      "ERNIE-Bot-turbo\t0.9173899230454107\t2.4244897959183676\n",
      "ERNIE-Bot-turbo\t0.7079405253554338\t2.4959183673469387\n",
      "ERNIE-Bot-turbo\t0.4197475117892281\t2.673469387755102\n",
      "ERNIE-Bot-turbo\t0.8722063351310373\t2.4244897959183676\n",
      "ERNIE-Speed\t0.3138439582719443\t2.8959183673469386\n",
      "ERNIE-Speed\t0.448023350389325\t2.8224489795918366\n",
      "ERNIE-Speed\t0.9165385520594768\t2.559183673469388\n",
      "ERNIE-Bot-turbo\t0.8672876273361033\t2.4653061224489794\n",
      "ERNIE-Speed\t0.4456239330358356\t2.840816326530612\n",
      "ERNIE-Bot-turbo\t0.30054691262379607\t2.589795918367347\n",
      "ERNIE-Bot-turbo\t0.6424633447338324\t2.5428571428571427\n",
      "ERNIE-Bot-turbo\t0.9439032508403612\t2.2081632653061223\n",
      "ERNIE-Speed\t0.6611344092317591\t2.679591836734694\n",
      "ERNIE-Speed\t0.19487693173821197\t2.9326530612244897\n",
      "ERNIE-Speed\t0.23741354477455734\t2.8401639344262297\n",
      "ERNIE-Bot-turbo\t0.553161701648807\t2.66734693877551\n",
      "ERNIE-Bot-turbo\t0.9050749392872927\t2.3755102040816327\n",
      "ERNIE-Speed\t0.5441689058420125\t2.695918367346939\n"
     ]
    }
   ],
   "source": [
    "for turn in context.history:\n",
    "    for trial in turn:\n",
    "        metrics = trial.metrics\n",
    "        config = trial.config\n",
    "        print(\"{}\\t{}\\t{}\".format(config['model'], config['temperature'], metrics['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 效果评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看到，搜索出的最佳配置 `{'temperature': 0.23879676911568554, 'model': 'ERNIE-Speed'}` 对应的分数约为 3.02。\n",
    "\n",
    "为了评估 SDK 推荐参数的性能表现，接下来我们将参考默认参数的 `ERNIE-Speed` 表现作为 baseline，而这数据也可以通过 SDK 快速获得，只需要调整搜索空间即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [04-01 14:08:19] launcher.py:108 [t:140289325633792]: turn 0 started...\n",
      "[INFO] [04-01 14:08:19] launcher.py:109 [t:140289325633792]: suggested config list: [{'model': 'ERNIE-Speed'}]\n",
      "[INFO] [04-01 14:08:19] dataset.py:883 [t:140289325633792]: list local dataset data by None\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:19] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:08:41] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:01] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:02] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:23] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:09:43] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:05] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:13] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:14] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:17] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:18] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:18] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:18] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:18] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:19] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:21] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:27] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:28] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:28] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:32] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:33] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:34] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:34] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:36] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:37] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:37] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:37] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:42] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:43] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:44] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:46] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:46] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:47] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:49] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:50] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:54] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:57] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:58] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:59] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:10:59] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:00] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:01] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:02] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:02] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:04] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:08] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:09] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:10] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:12] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:12] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:13] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:15] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:15] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:17] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:18] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:18] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:19] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:20] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:23] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:27] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:28] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:29] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:30] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:31] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:31] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:31] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:32] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:36] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:39] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:40] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:40] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:41] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:44] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:44] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:45] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:46] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:49] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:54] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:54] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:56] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:56] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:59] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:11:59] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:00] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:03] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:03] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:07] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:07] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:09] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:11] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:13] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:14] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:16] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:17] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:18] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:19] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:19] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:24] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:25] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:25] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:26] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:29] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:32] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:33] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:34] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:34] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:35] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:36] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:38] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:40] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:42] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:44] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:45] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:48] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:49] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:52] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:53] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:54] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:56] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:58] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:12:59] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:03] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:04] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:05] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:05] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:09] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:09] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:10] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:11] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:13] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:15] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:17] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:17] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:20] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:20] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:22] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:23] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:26] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:28] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:28] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:29] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:29] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:31] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:32] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:34] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:35] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:36] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:39] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:39] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:39] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:40] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:41] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:43] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:46] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:49] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:49] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:49] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:50] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:52] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:54] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:54] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:57] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:13:58] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:00] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:00] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:05] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:05] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:05] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:06] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:09] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:10] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:10] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:13] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:14] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:17] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:19] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:19] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:20] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:24] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:25] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:27] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:28] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:28] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:34] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:34] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:35] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:38] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:38] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:39] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:42] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:42] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:44] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:46] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:46] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:50] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:50] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:52] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:53] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:56] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:56] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:57] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:57] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:14:57] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:00] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:03] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:03] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:06] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:07] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:07] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:09] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:12] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:12] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:13] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:15] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:15] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:18] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:21] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:21] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:21] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:22] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:22] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:25] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:29] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:29] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:32] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:33] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:33] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:34] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:36] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:39] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:41] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:42] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:42] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:44] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:44] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:44] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:45] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:48] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:15:49] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:16:01] launcher.py:114 [t:140289325633792]: config: {'model': 'ERNIE-Speed'}, metrics: {'score': 2.4122448979591837, 'avg_prompt_tokens': 708.204081632653, 'avg_completion_tokens': 70.70204081632653, 'avg_total_tokens': 778.9061224489795, 'avg_req_latency': 12.200192404431956, 'avg_tokens_per_second': 63.843757264518786, 'avg_cost': 0.003398432653061224, 'total_cost': 0.8326159999999998, 'success_rate': 1.0, 'total_time': 462.4867479801178}\n",
      "[INFO] [04-01 14:16:01] launcher.py:92 [t:140289325633792]: max turn reached: 1\n",
      "[INFO] [04-01 14:16:01] launcher.py:102 [t:140289325633792]: tuning finished!\n",
      "[INFO] [04-01 14:16:01] launcher.py:104 [t:140289325633792]: best config: {'model': 'ERNIE-Speed'}\n"
     ]
    }
   ],
   "source": [
    "baseline = await qianfan.autotuner.run(\n",
    "    search_space={\n",
    "        \"model\": Categorical([\"ERNIE-Speed\"]),  # 这里仅保留唯一参数\n",
    "    },\n",
    "    dataset=dataset,\n",
    "    evaluator=LocalJudgeEvaluator(\n",
    "        model=ChatCompletion(model=\"ERNIE-Bot-4\")\n",
    "    ),\n",
    "    # 以下参数与上述保持一致\n",
    "    suggestor=\"random\",  # 搜索算法，目前仅支持 \"random\"，更多算法敬请期待\n",
    "    metrics=\"score\",     # 设定评估指标字段，与 Evaluator 输出对应\n",
    "    mode=\"max\",          # 设定评估指标最大化还是最小化\n",
    "    repeat=5,            # 重复推理次数，用于减少大模型输出随机性对结果准确性的干扰\n",
    "    max_turn=1,          # 设定最大尝试次数\n",
    "    log_dir= \"./log\",    # 日志目录\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4122448979591837"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.history[0][0].metrics['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在未进行参数调优时，模型得分仅为2.41，而调整参数后得分提升到了3.02，模型表现得到了巨大提升。\n",
    "\n",
    "为进一步了解模型的性能，接下来我们尝试与默认参数的 ERNIE-3.5 进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [04-01 14:21:57] launcher.py:108 [t:140289325633792]: turn 0 started...\n",
      "[INFO] [04-01 14:21:57] launcher.py:109 [t:140289325633792]: suggested config list: [{'model': 'ERNIE-3.5-8K'}]\n",
      "[INFO] [04-01 14:21:57] dataset.py:883 [t:140289325633792]: list local dataset data by None\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:57] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:21:58] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:22:28] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:23:13] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:24:14] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:15] openapi_requestor.py:377 [t:140289325633792]: async requesting llm api endpoint: /chat/completions\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:49] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:57] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:57] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:25:59] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:00] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:00] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:01] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:02] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:03] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:04] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:06] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:08] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:09] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:11] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:13] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:13] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:16] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:16] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:17] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:17] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:22] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:23] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:25] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:27] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:29] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:30] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:31] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:33] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:34] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:36] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:37] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:40] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:41] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:42] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:43] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:44] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:45] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:46] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:52] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:52] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:52] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:53] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:54] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:58] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:26:59] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:01] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:02] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:02] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:02] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:03] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:06] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:06] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:07] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:10] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:12] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:13] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:13] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:17] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:17] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:18] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:20] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:22] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:24] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:24] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:26] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:28] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:29] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:30] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:31] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:33] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:33] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:36] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:39] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:39] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:39] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:40] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:42] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:42] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:44] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:44] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:47] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:49] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:50] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:52] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:53] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:54] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:55] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:57] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:57] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:27:59] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:02] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:04] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:05] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:05] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:05] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:05] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:07] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:08] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:09] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:12] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:14] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:16] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:17] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:18] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:19] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:20] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:22] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:22] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:24] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:26] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:27] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:28] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:29] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:31] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:31] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:35] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:36] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:36] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:39] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:40] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:40] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:40] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:41] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:42] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:42] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:47] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:49] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:49] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:50] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:52] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:53] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:55] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:58] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:58] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:28:58] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:01] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:04] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:06] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:07] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:07] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:08] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:08] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:09] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:11] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:15] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:16] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:18] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:20] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:20] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:20] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:22] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:26] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:26] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:28] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:28] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:29] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:32] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:34] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:34] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:38] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:39] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:39] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:39] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:41] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:43] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:44] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:44] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:47] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:50] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:50] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:52] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:53] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:53] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:29:56] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:01] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:02] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:02] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:04] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:06] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:08] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:09] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:11] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:12] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:13] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:13] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:14] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:18] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:20] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:20] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:21] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:21] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:22] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:22] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:27] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:28] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:30] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:32] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:33] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:34] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:34] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:35] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:36] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:40] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:41] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:42] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:43] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:43] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:47] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:47] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:47] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:47] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:51] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:54] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:54] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:55] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:56] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:56] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:30:57] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:03] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:03] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:04] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:04] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:05] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:07] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:11] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:12] openapi_requestor.py:336 [t:140288010778368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:13] openapi_requestor.py:336 [t:140288002385664]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:15] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:15] openapi_requestor.py:336 [t:140288044349184]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:16] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:17] openapi_requestor.py:336 [t:140288027563776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:17] openapi_requestor.py:336 [t:140288137733888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:20] openapi_requestor.py:336 [t:140288052741888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:24] openapi_requestor.py:336 [t:140288019171072]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:25] openapi_requestor.py:336 [t:140288035956480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [04-01 14:31:39] launcher.py:114 [t:140289325633792]: config: {'model': 'ERNIE-3.5-8K'}, metrics: {'score': 3.0475206611570247, 'avg_prompt_tokens': 701.0619834710744, 'avg_completion_tokens': 63.264462809917354, 'avg_total_tokens': 764.3264462809917, 'avg_req_latency': 13.949587185235986, 'avg_tokens_per_second': 54.79204768797332, 'avg_cost': 0.009171917355371898, 'total_cost': 2.2196039999999995, 'success_rate': 0.9877551020408163, 'total_time': 581.2854714393616}\n",
      "[INFO] [04-01 14:31:39] launcher.py:92 [t:140289325633792]: max turn reached: 1\n",
      "[INFO] [04-01 14:31:39] launcher.py:102 [t:140289325633792]: tuning finished!\n",
      "[INFO] [04-01 14:31:39] launcher.py:104 [t:140289325633792]: best config: {'model': 'ERNIE-3.5-8K'}\n"
     ]
    }
   ],
   "source": [
    "eb35_result = await qianfan.autotuner.run(\n",
    "    search_space={\n",
    "        \"model\": Categorical([\"ERNIE-3.5-8K\"]),  \n",
    "    },\n",
    "    dataset=dataset,\n",
    "    evaluator=LocalJudgeEvaluator(\n",
    "        model=ChatCompletion(model=\"ERNIE-Bot-4\")\n",
    "    ),\n",
    "    # 以下参数与上述保持一致\n",
    "    suggestor=\"random\",  # 搜索算法，目前仅支持 \"random\"，更多算法敬请期待\n",
    "    metrics=\"score\",     # 设定评估指标字段，与 Evaluator 输出对应\n",
    "    mode=\"max\",          # 设定评估指标最大化还是最小化\n",
    "    repeat=5,            # 重复推理次数，用于减少大模型输出随机性对结果准确性的干扰\n",
    "    max_turn=1,          # 设定最大尝试次数\n",
    "    log_dir= \"./log\",    # 日志目录\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0475206611570247"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eb35_result.history[0][0].metrics['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Tokens Per Second\n",
      "ERNIE Speed:  63.843757264518786\n",
      "ERNIE 3.5:  54.79204768797332\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg Tokens Per Second\")\n",
    "print(\"ERNIE Speed: \", baseline.history[0][0].metrics['avg_tokens_per_second'])\n",
    "print(\"ERNIE 3.5: \", eb35_result.history[0][0].metrics['avg_tokens_per_second'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到推荐的参数配置 ERNIE Speed 表现已经能够与默认的 ERNIE 3.5 所媲美，而与此同时 ERNIE Speed 的价格仅为 ERNIE 3.5 的三分之一，能够在大幅降低成本的前提下保证性能几乎与更大的模型持平，还能获得更高的 token 吞吐量，有助于在实际应用场景下实现降本增效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
